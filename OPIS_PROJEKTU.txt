# Zautomatyzowany System Wspomagania Badań Naukowych

## Abstrakt
Niniejszy dokument opisuje zautomatyzowany system zaprojektowany w celu usprawnienia procesu badań naukowych. System orkiestruje wieloetapowy przepływ pracy, który obejmuje generowanie zapytań wspomagane przez AI, kompleksowe wyszukiwanie informacji w Internecie i akademickich bazach danych, ocenę jakości znalezionych wyników, inteligentne filtrowanie oraz ekstrakcję treści. Wykorzystuje architekturę modułową i solidny backend bazodanowy, aby zapewnić powtarzalność, zarządzać dużymi ilościami danych oraz dostarczać szczegółowe logowanie i raportowanie. System ten ma na celu znaczne zredukowanie ręcznego wysiłku związanego z przeglądem literatury i gromadzeniem informacji, pozwalając badaczom skupić się na analizie i syntezie.

## Spis Treści
- [Przepływ Pracy i Architektura Systemu](#przepływ-pracy-i-architektura-systemu)
  - [1. Inicjalizacja i Konfiguracja](#1-inicjalizacja-i-konfiguracja)
  - [2. Generowanie Zapytań (Opcjonalne)](#2-generowanie-zapytań-opcjonalne)
  - [3. Wyszukiwanie Informacji (Realizacja Wyszukiwania)](#3-wyszukiwanie-informacji-realizacja-wyszukiwania)
  - [4. Ocena Jakości](#4-ocena-jakości)
  - [5. Filtrowanie Wyników](#5-filtrowanie-wyników)
  - [6. Ekstrakcja i Przechowywanie Treści](#6-ekstrakcja-i-przechowywanie-treści)
  - [7. Raportowanie i Zarządzanie Danymi](#7-raportowanie-i-zarządzanie-danymi)
- [Przegląd Modułów](#przegląd-modułów)
- [Użycie Interfejsu Linii Komend (CLI)](#użycie-interfejsu-linii-komend-cli)
  - [Główny Przepływ Pracy](#główny-przepływ-pracy)
  - [Operacje Autonomiczne](#operacje-autonomiczne)
  - [Opcje Ogólne](#opcje-ogólne)
- [Instalacja i Konfiguracja](#instalacja-i-konfiguracja)
- [Struktura Projektu](#struktura-projektu)
- [Zmienne Środowiskowe](#zmienne-środowiskowe)
- [Baza Danych](#baza-danych)
- [Logowanie](#logowanie)

## Przepływ Pracy i Architektura Systemu
System działa poprzez sekwencyjny, ale elastyczny przepływ pracy, zarządzany przez główny moduł orkiestrujący (`src/main.py`). Każdy etap jest zaprojektowany tak, aby mógł być potencjalnie uruchamiany niezależnie lub jako część kompletnego potoku.

### 1. Inicjalizacja i Konfiguracja
- **Cel:** Przygotowanie systemu do działania.
- **Proces:**
    - Instancjonowana jest klasa `ResearchWorkflow`.
    - Ustanawiane jest połączenie z bazą danych SQLite (domyślnie: `data/research_db.db`) za pomocą modułu `Database`.
    - Inicjalizowane są moduły dostawców wyszukiwania (`InternetSearchModule`, `ResearchPapersModule`).
    - Konfigurowany jest kompleksowy system logowania, kierujący dane wyjściowe zarówno do konsoli, jak i do pliku logu z sygnaturą czasową w katalogu `logs/`.

### 2. Generowanie Zapytań (Opcjonalne)
- **Cel:** Rozszerzenie początkowego tematu badawczego na zestaw szczegółowych zapytań wyszukiwawczych.
- **Wyzwalacz:** Gdy podany jest parametr `--topic` bez określonego pliku `--queries-file`.
- **Proces (metoda `generate_queries_from_topic`, wykorzystująca `query_agent.generate_queries_programmatically`):**
    - Jako zalążek służy temat (`topic`) podany przez użytkownika.
    - Model AI (np. Google Gemini) jest wykorzystywany do generowania listy zróżnicowanych i konkretnych zapytań wyszukiwawczych związanych z tematem. Może to obejmować wstępne przeszukiwanie sieci przez agenta AI (kontrolowane przez `--pages-to-visit`).
    - Wygenerowane zapytania, wraz z oryginalnym tematem, są zapisywane do pliku JSON z sygnaturą czasową w katalogu `outputs/` (np. `outputs/query_agent_search_queries_<timestamp>.json`).
- **Alternatywa:** Jeśli podany jest plik `--queries-file`, lub jeśli nie podano tematu, a istnieje poprzedni plik zapytań, ten etap jest pomijany, a zapytania są ładowane bezpośrednio z określonego/najnowszego pliku JSON.

### 3. Wyszukiwanie Informacji (Realizacja Wyszukiwania)
- **Cel:** Zgromadzenie odpowiednich zasobów z różnych źródeł online na podstawie wygenerowanych/załadowanych zapytań.
- **Proces (metoda `execute_searches`):**
    - Iteruje po każdym zapytaniu z aktywnego zestawu zapytań.
    - **Wyszukiwanie w Internecie (przez `InternetSearchModule`):**
        - Wykonuje zapytanie za pomocą ogólnego API wyszukiwania internetowego (np. Tavily Search API).
        - Wyniki (tytuły, adresy URL, fragmenty) są parsowane i przechowywane w tabeli `web_resources` w bazie danych, powiązane z zapytaniem nadrzędnym. Obsługiwana jest deduplikacja zasobów.
    - **Wyszukiwanie Publikacji Naukowych (przez `ResearchPapersModule`):**
        - Wykonuje zapytanie za pomocą API wyszukiwania publikacji naukowych (np. Semantic Scholar API).
        - Szczegóły publikacji (tytuły, adresy URL, abstrakty, autorzy, rok publikacji) są parsowane i przechowywane w tabeli `web_resources`, również powiązane z zapytaniem nadrzędnym i deduplikowane.
    - Możliwe jest wprowadzenie konfigurowalnego opóźnienia między wywołaniami do różnych dostawców w celu przestrzegania limitów API.
    - Ten etap może być ograniczony do określonej liczby zapytań (`--max-queries`) lub zawężony tylko do źródeł internetowych (`--internet-only`) lub publikacji naukowych (`--papers-only`).

### 4. Ocena Jakości
- **Cel:** Ewaluacja trafności, wiarygodności i użyteczności pobranych wyników wyszukiwania.
- **Proces (metoda `run_quality_assessment`, wykorzystująca `QualityAssessmentModule`):**
    - Działa na wynikach wyszukiwania przechowywanych w bazie danych, które nie zostały jeszcze ocenione.
    - Dla każdego nieocenionego zasobu, model AI (np. Google Gemini) analizuje jego metadane (tytuł, fragment/abstrakt) w kontekście oryginalnego zapytania użytkownika lub tematu badawczego.
    - AI przypisuje oceny za trafność, wiarygodność i użyteczność. Te oceny, oraz potencjalnie tekstowe uzasadnienie, są przechowywane w tabeli `quality_assessments`, powiązanej z odpowiednim `web_resource`.
    - Ocena może być przeprowadzana partiami (`--assessment-batch-size`).
    - Obliczane są statystyki, takie jak średnie oceny i rozkłady ocen.

### 5. Filtrowanie Wyników
- **Cel:** Wybór najbardziej obiecujących i najwyższej jakości zasobów na podstawie ocen jakości.
- **Proces (przez funkcję `run_filtering_workflow`, wykorzystującą `ResultFilteringModule`):**
    - Ten etap zazwyczaj następuje po ocenie jakości.
    - Pobiera ocenione wyniki z bazy danych.
    - Na podstawie zdefiniowanych przez użytkownika progów procentowych (`--internet-filter-percent`, `--research-filter-percent`), wybiera X% najlepszych wyników z Internetu i Y% najlepszych wyników z publikacji naukowych.
    - Wybrane zasoby są oznaczane w bazie danych (np. przez ustawienie flagi `is_top_candidate` lub podobny mechanizm), aby zidentyfikować je do kolejnego etapu ekstrakcji treści.

### 6. Ekstrakcja i Przechowywanie Treści
- **Cel:** Pobranie i przechowanie pełnej treści tekstowej przefiltrowanych, wysokiej jakości zasobów.
- **Wyzwalacz:** Gdy użyta jest flaga `--run-fetching` w głównym przepływie pracy.
- **Proces (przez funkcję `process_filtered_results` z `main_part2.py`, wykorzystującą `ContentExtractionAgent`):**
    - Identyfikuje zasoby oznaczone jako najlepsze kandydatury przez etap filtrowania.
    - Dla każdego wybranego zasobu:
        - `ContentExtractionAgent` przechodzi do adresu URL zasobu.
        - Pobiera główną treść tekstową. Obejmuje to specjalistyczną obsługę dokumentów PDF (ekstrakcja tekstu) i stron HTML (ekstrakcja głównej treści artykułu, usuwanie elementów zbędnych).
        - Wyekstrahowana surowa i/lub oczyszczona treść jest przechowywana w tabeli `content_cache` w bazie danych, powiązana z `web_resource`.
    - Zaimplementowano solidną obsługę błędów dla problemów sieciowych, ograniczeń dostępu do treści lub błędów parsowania.

### 7. Raportowanie i Zarządzanie Danymi
- **Cel:** Dostarczenie podsumowania wykonania przepływu pracy i utrwalenie szczegółowych wyników.
- **Proces:**
    - **Raportowanie (metoda `generate_report`):**
        - Generowany jest kompleksowy raport tekstowy i drukowany na konsoli. Raport ten zawiera:
            - Ogólne parametry wykonania (temat, czas).
            - Statystyki dotyczące generowania zapytań, realizacji wyszukiwania (dla każdego dostawcy), oceny jakości (oceny, rozkłady) i filtrowania.
            - Podsumowanie zawartości bazy danych (całkowita liczba zapytań, zasobów).
            - Wszelkie błędy napotkane podczas przepływu pracy.
    - **Zapisywanie Wyników (metoda `save_results`):**
        - Szczegółowy obiekt JSON zawierający wszystkie dane wykonawcze, statystyki i wyniki z każdego etapu jest zapisywany do pliku z sygnaturą czasową w katalogu `outputs/` (np. `outputs/workflow_results_<timestamp>.json`).
    - **Baza Danych (moduł `Database`):**
        - Wszystkie trwałe dane (zapytania, zasoby, oceny, wyekstrahowana treść) są zarządzane w bazie danych SQLite. Baza danych zapewnia integralność danych, obsługuje deduplikację i umożliwia zapytania oraz analizę danych badawczych.
        - Dostępna jest opcja `--clear-db` do resetowania bazy danych przed nowym uruchomieniem przepływu pracy.

## Przegląd Modułów
System składa się z kilku kluczowych modułów Pythona, z których każdy jest odpowiedzialny za określoną część przepływu pracy:

- **`src/main.py`**: Centralny moduł orkiestrujący. Parsuje argumenty linii komend, inicjalizuje inne moduły i zarządza sekwencyjnym wykonywaniem etapów przepływu pracy badawczego.
- **`src/database.py`**: Zapewnia warstwę abstrakcji dla wszystkich interakcji z bazą danych SQLite. Obsługuje tworzenie schematu, wstawianie danych, zapytania, aktualizacje, logikę deduplikacji oraz dostarcza funkcje narzędziowe do pobierania statystyk i określonych podzbiorów danych.
- **`src/query_agent.py` (konkretnie funkcja `generate_queries_programmatically`):** Odpowiedzialny za przyjmowanie początkowego, ogólnego tematu badawczego i, przy użyciu modelu AI, generowanie listy bardziej szczegółowych i ukierunkowanych zapytań wyszukiwawczych.
- **`src/internet_search_provider.py`**: Implementuje logikę wyszukiwania w ogólnym Internecie za pomocą zewnętrznych API (np. Tavily). Pobiera wyniki wyszukiwania i przechowuje je w bazie danych.
- **`src/research_papers_provider.py`**: Implementuje logikę wyszukiwania w akademickich bazach danych publikacji za pomocą zewnętrznych API (np. Semantic Scholar). Pobiera metadane dotyczące publikacji naukowych i przechowuje je.
- **`src/quality_assessment_module.py`**: Wykorzystuje model AI do oceny jakości pobranych wyników wyszukiwania na podstawie kryteriów takich jak trafność, wiarygodność i użyteczność. Przechowuje te oceny w bazie danych.
- **`src/result_filtering_module.py`**: Filtruje ocenione wyniki wyszukiwania w celu zidentyfikowania najbardziej obiecujących kandydatów do ekstrakcji treści, na podstawie ocen jakości i zdefiniowanych przez użytkownika progów.
- **`src/browsing_agent.py` (klasa `ContentExtractionAgent`):** Obsługuje faktyczne pobieranie i ekstrakcję treści tekstowej ze stron internetowych i dokumentów PDF zidentyfikowanych przez moduł filtrowania.
- **`src/main_part2.py` (konkretnie funkcja `process_filtered_results`):** Orkiestruje proces pobierania treści dla wybranych, przefiltrowanych wyników, wykorzystując `ContentExtractionAgent`.
- **`src/config.py` (oraz plik `.env`):** Zarządza ustawieniami konfiguracyjnymi, w tym kluczami API, ścieżkami do bazy danych i innymi parametrami operacyjnymi, głównie poprzez zmienne środowiskowe.
- **`src/gemini_client.py` (lub podobny klient AI):** Zapewnia interfejs do modelu AI (np. Google Gemini) używanego do generowania zapytań i oceny jakości.

## Użycie Interfejsu Linii Komend (CLI)
Głównym interfejsem do systemu jest skrypt `src/main.py`.

### Główny Przepływ Pracy
Aby uruchomić kompletny przepływ pracy (generowanie zapytań, wyszukiwanie, ocena, filtrowanie i pobieranie treści):
```bash
python src/main.py --topic "Twój Temat Badawczy" --run-fetching [--clear-db] [inne opcje]
```
Przykład:
```bash
python src/main.py --topic "Zastosowania AI w diagnostyce medycznej" --max-queries 10 --internet-filter-percent 20 --research-filter-percent 15 --run-fetching --clear-db
```

### Operacje Autonomiczne
Pewne części przepływu pracy mogą być uruchamiane niezależnie:

- **Uruchom tylko Ocenę Jakości (na istniejących, nieocenionych wynikach):**
  ```bash
  python src/main.py --run-assessment [--assessment-batch-size <N>]
  ```
- **Uruchom tylko Filtrowanie Wyników (na istniejących, ocenionych wynikach):**
  ```bash
  python src/main.py --run-filtering [--internet-filter-percent <P1>] [--research-filter-percent <P2>]
  ```

### Opcje Ogólne
- `--topic <str>`: Określa początkowy temat badawczy. Wymagane, jeśli nie używasz `--queries-file` do nowego wyszukiwania, lub jeśli uruchamiasz pełny przepływ pracy obejmujący generowanie zapytań.
- `--queries-file <Path>`: Ścieżka do pliku JSON zawierającego predefiniowane zapytania wyszukiwawcze. Jeśli podane, generowanie zapytań jest pomijane. Jeśli nie podane, system może użyć najnowszego wygenerowanego pliku zapytań z `outputs/` lub wygenerować nowe, jeśli podano temat.
- `--db-path <str>`: Określa ścieżkę do pliku bazy danych SQLite (domyślnie: `data/research_db.db`).
- `--clear-db`: Usuwa wszystkie dane z bazy danych przed rozpoczęciem przepływu pracy. Używaj ostrożnie.
- `--max-queries <int>`: Ogranicza liczbę zapytań wyszukiwawczych do przetworzenia z listy zapytań.
- `--internet-only`: Ogranicza wyszukiwania tylko do dostawcy wyszukiwania internetowego.
- `--papers-only`: Ogranicza wyszukiwania tylko do dostawcy publikacji naukowych.
- `--pages-to-visit <int>`: Dla generowania zapytań AI, liczba stron internetowych, które agent może odwiedzić w celu zebrania informacji do tworzenia zapytań (domyślnie: 5).
- `--assessment-batch-size <int>`: Liczba wyników do oceny w jednej partii podczas fazy oceny jakości (domyślnie: 10).
- `--run-assessment`: Flaga do jawnego uruchomienia etapu oceny jakości. W pełnym przepływie pracy, ocena jest zazwyczaj uruchamiana domyślnie po wyszukiwaniu. Ta flaga jest przydatna do uruchamiania oceny jako samodzielnego etapu.
- `--run-filtering`: Flaga do jawnego uruchomienia etapu filtrowania wyników.
    - `--internet-filter-percent <float>`: Procent najlepszych ocenionych wyników wyszukiwania internetowego do zachowania po filtrowaniu (domyślnie: 10.0).
    - `--research-filter-percent <float>`: Procent najlepszych ocenionych wyników publikacji naukowych do zachowania po filtrowaniu (domyślnie: 10.0).
- `--run-fetching`: Flaga umożliwiająca etap ekstrakcji i przechowywania treści dla przefiltrowanych wyników. Zazwyczaj uruchamiana po wyszukiwaniu, ocenie i filtrowaniu.

## Instalacja i Konfiguracja
1. **Sklonuj repozytorium:**
   ```bash
   git clone <adres-url-repozytorium>
   cd <katalog-repozytorium>
   ```
2. **Zainstaluj zależności:**
   ```bash
   pip install -r requirements.txt
   ```
3. **Skonfiguruj zmienne środowiskowe:**
   Skopiuj `.env.example` do `.env`:
   ```bash
   cp .env.example .env
   ```
   Edytuj plik `.env` i dodaj swoje klucze API oraz inne konfiguracje:
   - `GEMINI_API_KEY`: Twój klucz API Google Gemini.
   - `TAVILY_API_KEY`: Twój klucz API Tavily Search (jeśli używany jako dostawca wyszukiwania internetowego).
   - `DATABASE_PATH`: Ścieżka do pliku bazy danych SQLite (np. `data/research_db.db`).
   - `MAX_CONTENT_LENGTH`: Maksymalna długość treści do przetworzenia.
   - (Dodaj inne odpowiednie zmienne środowiskowe)

4. **(Opcjonalnie) Przetestuj konfigurację:**
   Jeśli dostarczono skrypt `test_setup.py`:
   ```bash
   python test_setup.py
   ```

## Struktura Projektu
```
post-diploma/
├── src/                    # Kod źródłowy
│   ├── main.py             # Główny moduł orkiestrujący i CLI
│   ├── database.py         # Moduł operacji na bazie danych
│   ├── query_agent.py      # Generowanie zapytań wspomagane przez AI
│   ├── internet_search_provider.py # Integracja wyszukiwania internetowego
│   ├── research_papers_provider.py # Integracja wyszukiwania publikacji naukowych
│   ├── quality_assessment_module.py # Ocena jakości wspomagana przez AI
│   ├── result_filtering_module.py # Filtrowanie ocenionych wyników
│   ├── browsing_agent.py   # Ekstrakcja treści ze stron internetowych/PDF
│   ├── main_part2.py       # Orkiestracja pobierania treści
│   ├── gemini_client.py    # Klient API Google Gemini
│   ├── config.py           # Zarządzanie konfiguracją (ładowanie .env)
│   └── ...                 # Inne moduły narzędziowe lub pod-pakiety
├── data/                   # Domyślny katalog dla plików bazy danych (np. research_db.db)
│                           # Ten katalog jest zazwyczaj w .gitignore
├── logs/                   # Pliki logów generowane podczas wykonywania przepływu pracy
│                           # Ten katalog jest zazwyczaj w .gitignore
├── outputs/                # Pliki wyjściowe (wygenerowane zapytania JSON, wyniki przepływu pracy JSON)
│                           # Ten katalog jest zazwyczaj w .gitignore
├── tests/                  # Testy jednostkowe i integracyjne
├── requirements.txt        # Zależności pakietów Pythona
├── .gitignore              # Określa celowo nieśledzone pliki
├── .env.example            # Szablon dla zmiennych środowiskowych
├── .env                    # Lokalne zmienne środowiskowe (powinny być w .gitignore)
└── README.md               # Ten plik
```

## Zmienne Środowiskowe
System opiera się na zmiennych środowiskowych do konfiguracji, głównie kluczy API i ścieżek plików. Są one zazwyczaj definiowane w pliku `.env` w głównym katalogu projektu.
Kluczowe zmienne to:
- `GEMINI_API_KEY`: Niezbędny do generowania zapytań i oceny jakości wspomaganej przez AI.
- `TAVILY_API_KEY`: (Lub inne klucze API dostawców wyszukiwania) Wymagane do wyszukiwania informacji.
- `DATABASE_PATH`: Definiuje lokalizację bazy danych SQLite.
- `LOG_LEVEL`: Kontroluje szczegółowość logowania (np. INFO, DEBUG).
- `MAX_QUERIES_PER_TOPIC`: Domyślny limit dla zapytań generowanych przez AI.
- `MAX_RESULTS_PER_QUERY`: Domyślny limit dla wyników pobieranych na zapytanie przez dostawców wyszukiwania.

## Baza Danych
System wykorzystuje bazę danych SQLite do przechowywania wszystkich trwałych danych. Schemat jest definiowany i zarządzany przez moduł `src/database.py`. Kluczowe tabele to:
- `search_queries`: Przechowuje początkowe tematy i wygenerowane zapytania wyszukiwawcze.
- `web_resources`: Przechowuje metadane dotyczące odkrytych zasobów internetowych (adresy URL, tytuły, fragmenty, źródło itp.) zarówno z wyszukiwań internetowych, jak i publikacji naukowych.
- `quality_assessments`: Przechowuje wygenerowane przez AI oceny (trafność, wiarygodność, użyteczność) dla każdego ocenionego zasobu internetowego.
- `content_cache`: Przechowuje wyekstrahowaną treść tekstową z pobranych stron internetowych i plików PDF.
Relacje między tymi tabelami umożliwiają śledzenie danych od początkowego zapytania do wyekstrahowanej treści. Baza danych implementuje również mechanizmy deduplikacji zapytań i zasobów.

## Logowanie
Kompleksowe logowanie jest zaimplementowane w całym systemie:
- Logi są wysyłane zarówno do konsoli, jak i do pliku z sygnaturą czasową w katalogu `logs/` (np. `logs/main_workflow_RRRRMMDD_GGMMSS.log`).
- Poziomy logowania można konfigurować (np. za pomocą zmiennej środowiskowej lub pliku konfiguracyjnego) w celu kontrolowania szczegółowości.
- Logi zawierają sygnatury czasowe, poziomy ważności oraz szczegółowe komunikaty o operacjach, sukcesach, ostrzeżeniach i błędach, ułatwiając debugowanie i monitorowanie przepływu pracy.
