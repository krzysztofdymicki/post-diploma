[
  {
    "id": 162,
    "url": "https://www.kapiche.com/blog/sentiment-analysis-tools",
    "title": "Sentiment Analysis Tools: 15 Top Picks Reviewed for Pricing ... - Kapiche",
    "snippet": "Top 15 Sentiment Analysis Tools Reviewed: Features, Pricing, Pros & Cons. In our extensive analysis, we evaluated 15 of the most popular sentiment analysis tools on the market. We looked at key features, pricing models, and the pros and cons of each tool to help you make an informed decision. Tool 1: Kapiche",
    "source_type": "internet",
    "pdf_url": null,
    "weighted_average_score": 4.3,
    "relevance_score": 5,
    "credibility_score": 3,
    "solidity_score": 4,
    "overall_usefulness_score": 5
  },
  {
    "id": 94,
    "url": "https://blog.hootsuite.com/social-media-sentiment-analysis-tools/",
    "title": "12 social media sentiment analysis tools for 2025 - Hootsuite",
    "snippet": "2. Talkwalker. Talkwalker is another great all-around social media monitoring tool.It powers the sentiment features in Hootsuite and is also available as a standalone platform with lots of functionality (like video engagement analysis, text analysis, and social listening tools).. Talkwalker analyzes sentiment from 150 million data sources, including over 30 social platforms, blogs, forums ...",
    "source_type": "internet",
    "pdf_url": null,
    "weighted_average_score": 4.0,
    "relevance_score": 4,
    "credibility_score": 4,
    "solidity_score": 4,
    "overall_usefulness_score": 4
  },
  {
    "id": 109,
    "url": "https://doi.org/10.1145/3532203",
    "title": "Engineers, Aware! Commercial Tools Disagree on Social Media Sentiment: Analyzing the Sentiment Bias of Four Major Tools",
    "snippet": "Large commercial sentiment analysis tools are often deployed in software engineering due to their ease of use. However, it is not known how accurate these tools are, and whether the sentiment ratings given by one tool agree with those given by another tool. We use two datasets - (1) NEWS consisting of 5,880 news stories and 60K comments from four social media platforms: Twitter, Instagram, YouTube, and Facebook; and (2) IMDB consisting of 7,500 positive and 7,500 negative movie reviews - to investigate the agreement and bias of four widely used sentiment analysis (SA) tools: Microsoft Azure (MS), IBM Watson, Google Cloud, and Amazon Web Services (AWS). We find that the four tools assign the same sentiment on less than half (48.1%) of the analyzed content. We also find that AWS exhibits neutrality bias in both datasets, Google exhibits bi-polarity bias in the NEWS dataset but neutrality bias in the IMDB dataset, and IBM and MS exhibit no clear bias in the NEWS dataset but have bi-polarity bias in the IMDB dataset. Overall, IBM has the highest accuracy relative to the known ground truth in the IMDB dataset. Findings indicate that psycholinguistic features - especially affect, tone, and use of adjectives - explain why the tools disagree. Engineers are urged caution when implementing SA tools for applications, as the tool selection affects the obtained sentiment labels.",
    "source_type": "paper",
    "pdf_url": "https://osuva.uwasa.fi/bitstream/10024/14421/2/Osuva_Jung_Salminen_Jansen_2022.pdf",
    "weighted_average_score": 5.0,
    "relevance_score": 5,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 5
  },
  {
    "id": 173,
    "url": "https://doi.org/10.1080/19312458.2019.1671966",
    "title": "What’s the Tone? Easy Doesn’t Do It: Analyzing Performance and Agreement Between Off-the-Shelf Sentiment Analysis Tools",
    "snippet": "ABSTRACT This article scrutinizes the method of automated content analysis to measure the tone of news coverage. We compare a range of off-the-shelf sentiment analysis tools to manually coded economic news as well as examine the agreement between these dictionary approaches themselves. We assess the performance of five off-the-shelf sentiment analysis tools and two tailor-made dictionary-based approaches. The analyses result in five conclusions. First, there is little overlap between the off-the-shelf tools; causing wide divergence in terms of tone measurement. Second, there is no stronger overlap with manual coding for short texts (i.e., headlines) than for long texts (i.e., full articles). Third, an approach that combines individual dictionaries achieves a comparably good performance. Fourth, precision may increase to acceptable levels at higher levels of granularity. Fifth, performance of dictionary approaches depends more on the number of relevant keywords in the dictionary than on the number of valenced words as such; a small tailor-made lexicon was not inferior to large established dictionaries. Altogether, we conclude that off-the-shelf sentiment analysis tools are mostly unreliable and unsuitable for research purposes – at least in the context of Dutch economic news – and manual validation for the specific language, domain, and genre of the research project at hand is always warranted.",
    "source_type": "paper",
    "pdf_url": "https://www.tandfonline.com/doi/pdf/10.1080/19312458.2019.1671966?needAccess=true",
    "weighted_average_score": 5.0,
    "relevance_score": 5,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 5
  },
  {
    "id": 179,
    "url": "https://doi.org/10.1002/widm.1333",
    "title": "Sentiment analysis for mining texts and social networks data: Methods and tools",
    "snippet": "Social networks (SNs) represent an established environment in which users share daily emotions and opinions. Therefore, they have become an essential source of big data related to sentiment/opinion sphere. Sentiment analysis (SA) aims to extract sentiments, emotions or opinions from texts, made available by different data sources like SNs. This review presents a depth study relative to the methods and the main tools for SA. The analysis was performed by defining four criteria and several variables to compare 24 tools with objective criteria. Specifically, the tools have been analyzed and tested to verify their usability, flexibility of use, and other specifications related to the type of analysis performed. The majority of tools can detect positive, negative, and neutral polarity, while few tools only detect positive and negative polarity. Moreover, seven tools were able to recognize emotions, and only one provides a visual map for geo‐referenced data. Except for one, remaining 23 tools offer service through the web interface. Finally, only nine tools provide both application program interfaces and a client for common programming languages to allow potential developer end‐users to integrate a specific SA tool into their application. Differently, from other recent surveys, the paper presents and discusses both methods and tools for analyzing texts and SN data sources to extract sentiment. Moreover, it contains a comprehensive comparison with other recent surveys. The comparative analysis of the tools completed according to objective criteria allows to highlight some limits on main tools that need to be faced with enhancing the end‐user experience.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 5.0,
    "relevance_score": 5,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 5
  },
  {
    "id": 186,
    "url": "https://doi.org/10.70000/cj.2024.74.609",
    "title": "Open source sentiment analysis software: an evaluative study of functions and capabilities",
    "snippet": "\"Open-source sentiment analysis software is a powerful tool for understanding emotions and opinions, enabling the improvement of services and products. All organizations can benefit from these tools to gain insights and make informed decisions based on data analysis. This allows for the development of services or products to meet customer needs, as data has become a valuable asset for organizations. Customer opinions and feedback are among the most important data. The interest in sentiment analysis methods has become even more critical with the increasing prevalence of social media. Through these platforms, users freely share their opinions, making them a rich source of information about public sentiment towards various issues and topics. As a result, numerous automated programs for sentiment analysis have emerged to understand public opinion trends and improve products and services for a comprehensive outcome and to support decision-making. From this perspective, this study conducts an evaluation and comparative analysis of these software programs to identify the differences between them in terms of their features and to assess the available capabilities. A comparative analysis of 38 open-source sentiment analysis software programs was conducted, and it was found that AssemblyAi is the best software, ranking first according to the proposed evaluation criteria.\"",
    "source_type": "paper",
    "pdf_url": "https://doi.org/10.70000/cj.2024.74.609",
    "weighted_average_score": 5.0,
    "relevance_score": 5,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 5
  },
  {
    "id": 84,
    "url": "https://doi.org/10.63125/61pqqq54",
    "title": "AI-POWERED SENTIMENT ANALYSIS IN DIGITAL MARKETING: A REVIEW OF CUSTOMER FEEDBACK LOOPS IN IT SERVICES",
    "snippet": "This systematic review critically examines the evolving role of AI-powered sentiment analysis in optimizing digital marketing strategies, with a specific focus on its application within customer feedback loops in IT service environments. In the era of data-driven marketing, the ability to decode consumer emotions from unstructured textual sources—such as social media, product reviews, helpdesk transcripts, and chat logs—has become increasingly valuable for enhancing personalization, engagement, and service responsiveness. Adhering to the PRISMA 2020 methodology, this review rigorously analyzed 87 peer-reviewed articles published between 2010 and 2024, encompassing diverse disciplines including artificial intelligence, natural language processing, marketing analytics, and service operations. The findings reveal that while traditional stochastic models like Support Vector Machines remain widely used due to their computational efficiency and interpretability, deep learning architectures—particularly CNNs, LSTMs, and GRUs—have demonstrated superior performance in managing complex, context-rich sentiment patterns. Moreover, transformer-based models such as BERT and RoBERTa have emerged as state-of-the-art tools, excelling in multilingual sentiment interpretation and capturing nuanced emotional dynamics in long-form or domain-specific feedback. The integration of these models into customer feedback loops has enabled real-time marketing decision-making, automated customer relationship management, and sentiment-driven content optimization. However, the review also identifies key gaps, notably the underutilization of internal enterprise data sources and the lack of comprehensive adoption of explainable AI practices. Increasing scrutiny under data protection regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) has further underscored the need for transparency, user consent, and ethical handling of inferred emotional data. Overall, this review contributes to the growing body of literature by offering a comprehensive evaluation of current technologies, identifying operational challenges, and highlighting the need for ethically aligned and context-aware sentiment analytics frameworks in digital marketing ecosystems, particularly within the IT services sector.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 4.27,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 4
  },
  {
    "id": 201,
    "url": "https://doi.org/10.1109/ICOSST51357.2020.9333119",
    "title": "Comparative Analysis and Enhancement of Sentiment Intensity Based Tools",
    "snippet": "Sentiment intensity of a text indicates the strength of its association with positive or negative sentiment, which is measured in terms of continuous numerical value. Predicting sentiment intensities can achieve more fine-grained sentiment analysis as compared to polarity classification. This research paper is focused on sentiment intensity detection and comprises of two main parts; to find and address loopholes through rule-based approach by making a comparison of existing sentiment intensity measuring tools and a technique to enhance sentiment intensity-based classification by a lexicon-based approach to incorporate Word Sense Disambiguation (WSD). There is a strong need to conduct a thorough comparison of sentiment intensity-based methods to understand the potential limitations, advantages, and disadvantages of popular methods. This article aims at filling this gap by presenting a benchmark comparison of five popular sentiment intensity-based analysis methods. The second part is focused on constructing a human-annotated lexicon of adjectives paired with nouns triggering ambiguity, mapped with intensity as well as polarity. This Lexicon is further integrated with Enhanced Vader to measure performance; the result has outperformed with classification f-score 72%, much better than sentiment detection without lexicon.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 4.27,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 4
  },
  {
    "id": 297,
    "url": "https://doi.org/10.54660/.ijfmr.2023.4.1.269-274",
    "title": "Systematic Review of Sentiment Analysis and Market Research Applications in Digital Platform Strategy",
    "snippet": "This systematic review explores the integration of sentiment analysis and market research in shaping digital platform strategies. The paper synthesizes current literature on the tools, techniques, and applications of sentiment analysis, highlighting advancements in Natural Language Processing (NLP) and Machine Learning (ML) algorithms, such as lexicon-based approaches, deep learning models, and transformer-based architectures. It examines how these techniques contribute to strategic decisions related to customer feedback loops, brand perception, user engagement optimization, and market research methods such as A/B testing, surveys, and behavioral analytics. The review identifies key trends in the field, such as the rise of multimodal sentiment analysis and the need for more longitudinal studies. It also addresses the ethical considerations of data privacy, algorithmic bias, and the responsible use of insights. This paper offers significant implications for both practitioners and researchers, providing a comprehensive understanding of the current state of sentiment analysis and market research in digital platform strategy, while also suggesting areas for future inquiry and improvement.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 4.27,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 4
  },
  {
    "id": 13,
    "url": "https://doi.org/10.58247/jdset-2022-0502-12",
    "title": "A COMPARISON STUDY ON TEXT MINING AND SENTIMENT ANALYSIS FEATURES AND FUNCTIONS USING SAS ENTERPRISE MINER, PYTHON AND R",
    "snippet": "TTwitter has allowed textual data to be collected using Text Mining and Sentiment Analysis techniques in the age of social media in which user-generated content becomes redundant. However, due to some inconsistencies, Text Cleaning plays an important role before Text Mining and Sentiment Analysis techniques can be conducted. Hence, this study is conducted to discover the capabilities of Text Cleaning, Text Mining and Sentiment Analysis in three different data mining tools: SAS® Text Miner (proprietary text mining tool), Python and R programming (open-source text mining tools). These data mining tools were used to conduct the Text Cleaning, Text Mining and Sentiment Analysis and their capabilities such as features, functions and characteristics were evaluated and investigated, to conduct this comparison study. All the proposed research objectives were met successfully even with the given limitation. A movie critique Dictionary is one of the major theoretical implications of this research. Based on our analysis and results, developers or educational practitioners can discover what is important and what is unimportant when conducting Text Mining and Sentiment Analysis. They will also obtain insights and guidance on how to conduct Text Mining and Sentiment Analysis using SAS Enterprise Miner, Python and R.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 4.0,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 4,
    "overall_usefulness_score": 4
  },
  {
    "id": 14,
    "url": null,
    "title": "Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets",
    "snippet": "We report results of a comparison of the accuracy of crowdworkers and seven Natural Language Processing (NLP) toolkits in solving two important NLP tasks, named-entity recognition (NER) and entity-level sentiment (ELS) analysis. We here focus on a challenging dataset, 1,000 political tweets that were collected during the U.S. presidential primary election in February 2016. Each tweet refers to at least one of four presidential candidates, i.e., four named entities. The groundtruth, established by experts in political communication, has entity-level sentiment information for each candidate mentioned in the tweet. We tested several commercial and open-source tools. Our experiments show that, for our dataset of political tweets, the most accurate NER system, Google Cloud NL, performed almost on par with crowdworkers, but the most accurate ELS analysis system, TensiStrength, did not match the accuracy of crowdworkers by a large margin of more than 30 percent points.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 4.0,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 4,
    "overall_usefulness_score": 4
  },
  {
    "id": 145,
    "url": "https://doi.org/10.58247/jdset-2022-0502-12",
    "title": "A COMPARISON STUDY ON TEXT MINING AND SENTIMENT ANALYSIS FEATURES AND FUNCTIONS USING SAS ENTERPRISE MINER, PYTHON AND R",
    "snippet": "TTwitter has allowed textual data to be collected using Text Mining and Sentiment Analysis techniques in the age of social media in which user-generated content becomes redundant. However, due to some inconsistencies, Text Cleaning plays an important role before Text Mining and Sentiment Analysis techniques can be conducted. Hence, this study is conducted to discover the capabilities of Text Cleaning, Text Mining and Sentiment Analysis in three different data mining tools: SAS® Text Miner (proprietary text mining tool), Python and R programming (open-source text mining tools). These data mining tools were used to conduct the Text Cleaning, Text Mining and Sentiment Analysis and their capabilities such as features, functions and characteristics were evaluated and investigated, to conduct this comparison study. All the proposed research objectives were met successfully even with the given limitation. A movie critique Dictionary is one of the major theoretical implications of this research. Based on our analysis and results, developers or educational practitioners can discover what is important and what is unimportant when conducting Text Mining and Sentiment Analysis. They will also obtain insights and guidance on how to conduct Text Mining and Sentiment Analysis using SAS Enterprise Miner, Python and R.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 4.0,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 4,
    "overall_usefulness_score": 4
  },
  {
    "id": 193,
    "url": null,
    "title": "Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets",
    "snippet": "We report results of a comparison of the accuracy of crowdworkers and seven Natural Language Processing (NLP) toolkits in solving two important NLP tasks, named-entity recognition (NER) and entity-level sentiment (ELS) analysis. We here focus on a challenging dataset, 1,000 political tweets that were collected during the U.S. presidential primary election in February 2016. Each tweet refers to at least one of four presidential candidates, i.e., four named entities. The groundtruth, established by experts in political communication, has entity-level sentiment information for each candidate mentioned in the tweet. We tested several commercial and open-source tools. Our experiments show that, for our dataset of political tweets, the most accurate NER system, Google Cloud NL, performed almost on par with crowdworkers, but the most accurate ELS analysis system, TensiStrength, did not match the accuracy of crowdworkers by a large margin of more than 30 percent points.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 4.0,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 4,
    "overall_usefulness_score": 4
  },
  {
    "id": 197,
    "url": "https://doi.org/10.58247/jdset-2022-0502-12",
    "title": "A COMPARISON STUDY ON TEXT MINING AND SENTIMENT ANALYSIS FEATURES AND FUNCTIONS USING SAS ENTERPRISE MINER, PYTHON AND R",
    "snippet": "TTwitter has allowed textual data to be collected using Text Mining and Sentiment Analysis techniques in the age of social media in which user-generated content becomes redundant. However, due to some inconsistencies, Text Cleaning plays an important role before Text Mining and Sentiment Analysis techniques can be conducted. Hence, this study is conducted to discover the capabilities of Text Cleaning, Text Mining and Sentiment Analysis in three different data mining tools: SAS® Text Miner (proprietary text mining tool), Python and R programming (open-source text mining tools). These data mining tools were used to conduct the Text Cleaning, Text Mining and Sentiment Analysis and their capabilities such as features, functions and characteristics were evaluated and investigated, to conduct this comparison study. All the proposed research objectives were met successfully even with the given limitation. A movie critique Dictionary is one of the major theoretical implications of this research. Based on our analysis and results, developers or educational practitioners can discover what is important and what is unimportant when conducting Text Mining and Sentiment Analysis. They will also obtain insights and guidance on how to conduct Text Mining and Sentiment Analysis using SAS Enterprise Miner, Python and R.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 4.0,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 4,
    "overall_usefulness_score": 4
  },
  {
    "id": 350,
    "url": "https://doi.org/10.17762/ijritcc.v10i12.5840",
    "title": "Deciphering Voice of the Customer using Text Analytics and Sentiment Analysis: An Interpretable Review Rating Prediction using RoBERTa",
    "snippet": "In this era of cut-throat competition among traditional and newer digital organizations, capturing, listening, and understanding customer voices are critical for success in the marketplace. The challenge to decipher the voice of the customer (VOC) has multiplied many times today, as now the number of customer reviews are present in multiple  platforms and the data to be analyzed is huge. Sentiment analysis, and text analytics using machine learning, deep  learning tools and transformer-based tools can be applied to gather meaningful insights from these data. This paper applies the traditional machine learning tools of the Naive Bayes classifier, Random Forest and AdaBoost to predict  the customer review ratings. These results are compared with deep learning methods of CNN, RNN and Bi-LSTM and  transformer-based approaches of BERT, DistilBERT and RoBERTa. The results show that RoBERTa has the highest accuracy among these methods. Paper also uses the explainable AI tool of LIME to understand the customer sentiments deeper in terms of why customers are giving a particular rating to the product. Business organizations will continue to use more and more AI tools to understand the customer feedback and the attempt in this paper is to learn how we can make predictions faster and more accurately.",
    "source_type": "paper",
    "pdf_url": "https://ijritcc.org/index.php/ijritcc/article/download/5840/5552",
    "weighted_average_score": 4.0,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 4,
    "overall_usefulness_score": 4
  },
  {
    "id": 155,
    "url": "https://doi.org/10.5220/0007759306440651",
    "title": "\"It's Modern Day Presidential! An Evaluation of the Effectiveness of Sentiment Analysis Tools on President Donald Trump's Tweets\"",
    "snippet": "This paper reports on an evaluation of five commonly used, lexicon-based sentiment analysis tools (MeaningCloud, ParallelDots, Repustate, RSentiment for R, SentiStrength), tested for accuracy against a collection of Trump’s tweets spanning from election day November 2016 to one year post inauguration (January 2018). Repustate was found to be the most accurate at 67.53%. Our preliminary analysis suggests that this percentage reflects Trump’s frequent inclusion of both positive and negative sentiments in a single tweet. Additionally to providing an evaluative comparison of sentiment analysis tools, a summary of shared features of a number of existing datasets containing Twitter content along with a comprehensive discussion is also provided.",
    "source_type": "paper",
    "pdf_url": "https://doi.org/10.5220/0007759306440651",
    "weighted_average_score": 3.73,
    "relevance_score": 4,
    "credibility_score": null,
    "solidity_score": 4,
    "overall_usefulness_score": 3
  },
  {
    "id": 66,
    "url": "https://doi.org/10.54254/2755-2721/2024.20540",
    "title": "Sentiment Analysis and Rating Video Game Dimensions via NLP",
    "snippet": "Online gaming platforms generate vast amounts of user comments, which serve as valuable information for potential users and purchasers. Extracting meaningful guiding information from this data is crucial. As a Natural Language Processing technique (referred to as NLP), sentiment analysis technology demonstrates a high efficiency in accurately discerning the emotional tones within the comments, thereby objectively evaluating the advantages and disadvantages of gaming products based on user comments. In this paper, the comments of two games are analyzed, Grand Theft Auto V (referred to as GTAV) and Cyberpunk 2077 (referred to as 2077) on the Steam platform. Based on the self-built sentiment dictionary, keywords representing different sentiments were extracted by applying two different sentiment analysis models which are VADER and TEXTBLOB. Then we apply two models to classify the sentiments that the keywords express. Finally, the ratings of five dimensions (community, gameplay, storyline, sound and graphic) were obtained by transforming the sentiment analyze score, and the differences in the training results of different models were compared. The result analysis shows the performance difference between the VADER model and TEXTBLOB model through analyzing the standardized scores and the Pearson correlation coefficient. And it was particularly noted that the VADER model is better at capturing emotional changes in game reviews, while the TEXTBLOB model may not fully represent the users emotional inclination towards various aspects related to the game. The results reveal significant differences in model performance, providing insights into their effectiveness for gaming comment analysis.",
    "source_type": "paper",
    "pdf_url": null,
    "weighted_average_score": 3.53,
    "relevance_score": 3,
    "credibility_score": null,
    "solidity_score": 4,
    "overall_usefulness_score": 4
  },
  {
    "id": 107,
    "url": "https://doi.org/10.2196/50150",
    "title": "A Comparison of ChatGPT and Fine-Tuned Open Pre-Trained Transformers (OPT) Against Widely Used Sentiment Analysis Tools: Sentiment Analysis of COVID-19 Survey Data",
    "snippet": "Background Health care providers and health-related researchers face significant challenges when applying sentiment analysis tools to health-related free-text survey data. Most state-of-the-art applications were developed in domains such as social media, and their performance in the health care context remains relatively unknown. Moreover, existing studies indicate that these tools often lack accuracy and produce inconsistent results. Objective This study aims to address the lack of comparative analysis on sentiment analysis tools applied to health-related free-text survey data in the context of COVID-19. The objective was to automatically predict sentence sentiment for 2 independent COVID-19 survey data sets from the National Institutes of Health and Stanford University. Methods Gold standard labels were created for a subset of each data set using a panel of human raters. We compared 8 state-of-the-art sentiment analysis tools on both data sets to evaluate variability and disagreement across tools. In addition, few-shot learning was explored by fine-tuning Open Pre-Trained Transformers (OPT; a large language model [LLM] with publicly available weights) using a small annotated subset and zero-shot learning using ChatGPT (an LLM without available weights). Results The comparison of sentiment analysis tools revealed high variability and disagreement across the evaluated tools when applied to health-related survey data. OPT and ChatGPT demonstrated superior performance, outperforming all other sentiment analysis tools. Moreover, ChatGPT outperformed OPT, exhibited higher accuracy by 6% and higher F-measure by 4% to 7%. Conclusions This study demonstrates the effectiveness of LLMs, particularly the few-shot learning and zero-shot learning approaches, in the sentiment analysis of health-related survey data. These results have implications for saving human labor and improving efficiency in sentiment analysis tasks, contributing to advancements in the field of automated sentiment analysis.",
    "source_type": "paper",
    "pdf_url": "https://mental.jmir.org/2024/1/e50150/PDF",
    "weighted_average_score": 3.53,
    "relevance_score": 3,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 3
  },
  {
    "id": 239,
    "url": "https://doi.org/10.2196/50150",
    "title": "A Comparison of ChatGPT and Fine-Tuned Open Pre-Trained Transformers (OPT) Against Widely Used Sentiment Analysis Tools: Sentiment Analysis of COVID-19 Survey Data",
    "snippet": "Background Health care providers and health-related researchers face significant challenges when applying sentiment analysis tools to health-related free-text survey data. Most state-of-the-art applications were developed in domains such as social media, and their performance in the health care context remains relatively unknown. Moreover, existing studies indicate that these tools often lack accuracy and produce inconsistent results. Objective This study aims to address the lack of comparative analysis on sentiment analysis tools applied to health-related free-text survey data in the context of COVID-19. The objective was to automatically predict sentence sentiment for 2 independent COVID-19 survey data sets from the National Institutes of Health and Stanford University. Methods Gold standard labels were created for a subset of each data set using a panel of human raters. We compared 8 state-of-the-art sentiment analysis tools on both data sets to evaluate variability and disagreement across tools. In addition, few-shot learning was explored by fine-tuning Open Pre-Trained Transformers (OPT; a large language model [LLM] with publicly available weights) using a small annotated subset and zero-shot learning using ChatGPT (an LLM without available weights). Results The comparison of sentiment analysis tools revealed high variability and disagreement across the evaluated tools when applied to health-related survey data. OPT and ChatGPT demonstrated superior performance, outperforming all other sentiment analysis tools. Moreover, ChatGPT outperformed OPT, exhibited higher accuracy by 6% and higher F-measure by 4% to 7%. Conclusions This study demonstrates the effectiveness of LLMs, particularly the few-shot learning and zero-shot learning approaches, in the sentiment analysis of health-related survey data. These results have implications for saving human labor and improving efficiency in sentiment analysis tasks, contributing to advancements in the field of automated sentiment analysis.",
    "source_type": "paper",
    "pdf_url": "https://mental.jmir.org/2024/1/e50150/PDF",
    "weighted_average_score": 3.53,
    "relevance_score": 3,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 3
  },
  {
    "id": 4,
    "url": "https://doi.org/10.2196/50150",
    "title": "A Comparison of ChatGPT and Fine-Tuned Open Pre-Trained Transformers (OPT) Against Widely Used Sentiment Analysis Tools: Sentiment Analysis of COVID-19 Survey Data",
    "snippet": "Background Health care providers and health-related researchers face significant challenges when applying sentiment analysis tools to health-related free-text survey data. Most state-of-the-art applications were developed in domains such as social media, and their performance in the health care context remains relatively unknown. Moreover, existing studies indicate that these tools often lack accuracy and produce inconsistent results. Objective This study aims to address the lack of comparative analysis on sentiment analysis tools applied to health-related free-text survey data in the context of COVID-19. The objective was to automatically predict sentence sentiment for 2 independent COVID-19 survey data sets from the National Institutes of Health and Stanford University. Methods Gold standard labels were created for a subset of each data set using a panel of human raters. We compared 8 state-of-the-art sentiment analysis tools on both data sets to evaluate variability and disagreement across tools. In addition, few-shot learning was explored by fine-tuning Open Pre-Trained Transformers (OPT; a large language model [LLM] with publicly available weights) using a small annotated subset and zero-shot learning using ChatGPT (an LLM without available weights). Results The comparison of sentiment analysis tools revealed high variability and disagreement across the evaluated tools when applied to health-related survey data. OPT and ChatGPT demonstrated superior performance, outperforming all other sentiment analysis tools. Moreover, ChatGPT outperformed OPT, exhibited higher accuracy by 6% and higher F-measure by 4% to 7%. Conclusions This study demonstrates the effectiveness of LLMs, particularly the few-shot learning and zero-shot learning approaches, in the sentiment analysis of health-related survey data. These results have implications for saving human labor and improving efficiency in sentiment analysis tasks, contributing to advancements in the field of automated sentiment analysis.",
    "source_type": "paper",
    "pdf_url": "https://mental.jmir.org/2024/1/e50150/PDF",
    "weighted_average_score": 3.27,
    "relevance_score": 3,
    "credibility_score": null,
    "solidity_score": 5,
    "overall_usefulness_score": 2
  }
]